# ===============================================================
# ScholarScope 設定ファイル (TOML形式)
# ===============================================================

# --- プロジェクト設定 ---
# アプリケーションが使用する「ワークスペース」のルートディレクトリ。
# workspace_directory は、プロジェクトルートからの相対パスで指定
# "C:/Users/YourUser/Documents/MyResearch" のように絶対パスも指定可能。
workspace_directory = "my_workspace"


# --- パス設定 ---
# workspace_directoryを基準とした相対パスを指定します。
[paths]
# 文書ファイルを格納するフォルダ
documents_folder = "documents"

# アプリケーションが生成するデータを格納する隠しフォルダ
app_data_folder = ".scholarscope"

# ベクトルストアの永続化先 (app_data_folderからの相対パス)
vectorstore_folder = "vectorstore"

# BM25インデックスの永続化先 (app_data_folderからの相対パス)
bm25_index_folder = "bm25_index"

# 処理済みファイルのメタデータ記録ファイル (app_data_folderからの相対パス)
metadata_file = "metadata.json"

# システムログ・デバッグログの保存先ディレクトリ (プロジェクトルートからの相対パス)
system_log_directory = "logs"

# 検索履歴の記録ファイル (プロジェクトルートからの相対パス)
# ※この履歴記録はログ出力のON/OFFに関係なく常に残されます。
search_history_file = "logs/history.jsonl"

# ログファイル名 (system_log_directory内)
log_file = "app.log"

# --- モデル設定 ---
[models]
ollama_chat = "qwen3-4b-inst-iq4nl"
embedding = "Qwen3-Embedding-4B-f16"
# reranker = "BAAI/bge-reranker-v2-m3"
reranker = "ai_models/bge-reranker-v2-m3"

# --- UI・プロンプト設定 ---
[settings]
default_max_context_tokens = 10000
deduplication_threshold_for_prompt = 0.97

# AND検索結果の最大表示件数
and_search_retrieval_count = 200

# 1. 初期検索（Reranking前）の取得件数
# ★重要: ここは「reranker_input_count_deep（じっくりモード）」と同等か、それ以上に設定することを推奨します。
# 理由は以下の2点です：
# 1) Deep Searchモード選択時（例:60件）に、リランクに回すための元候補が足りなくなるのを防ぐため。
#    （※プログラム側でも自動拡張されますが、ここで明示的に確保しておくのが安全です）
# 2) リランキングで選ばれなかった候補を使って「バックフィル（穴埋め）」を行う際、
#    ここに余裕がないと埋めるための予備候補が枯渇してしまうため。
# 初期検索（BM25/ベクトル）自体の負荷は非常に軽いため、60〜80程度に増やしても速度への影響は軽微です。
bm25_retrieval_count = 120
vector_retrieval_count = 120

# 2. リランキング対象にする件数（片側）
# リランカーには、BM25とVectorからそれぞれこの件数ずつ渡されます。
# （例: 30 を設定 → BM25上位30件 + Vector上位30件 = 最大60件(重複除く) をリランク）
# ここを増やすと再現率（Recall）は上がりますが、リランク処理の時間が増加します。
#
# - reranker_input_count: 「標準モード」用。速度と精度のバランスが良い設定（推奨: 30）。
# - reranker_input_count_deep: 「じっくりモード」用。論点抽出やレポート作成など、
#   情報の取りこぼしを極力防ぎたい場合に使用します（推奨: 60 = 標準の2倍程度）。
reranker_input_count = 30
reranker_input_count_deep = 60

# 検索結果を最初に何件まで展開（開いた状態に）しておくか
initially_expanded_results_count = 1
# AND検索結果表示のテキスト分量
and_search_context_window_chars = 600
# エンベディング時、一度に処理するバッチサイズ（マシンスペックの余裕に応じて大きく設定）
embedding_batch_size = 180

# バックフィル（リランキング後の穴埋め）の判定基準
# BM25: トップスコアの何割まで許容するか (0.6 = 60%)
backfill_bm25_score_drop_ratio = 0.6
# Vector: トップ距離(distance)の何倍まで許容するか (1.5倍)
backfill_vector_distance_expansion_ratio = 1.5

[settings.chunking.bm25]
size = 600
overlap = 100

[settings.chunking.vector]
size = 600
overlap = 100

[ui]
must_keywords_placeholder = "スペース区切りでAND検索、-で除外検索"
semantic_query_placeholder = '''例: ◯◯という概念について教えて。
'''
# AIによる回答の典拠表示のコンテキスト番号表示ょのON/OFF
show_citation_context_numbers = false

# --- プロンプト設定 ---
# ''' で囲まれたブロック内は、インデントを気にせず自由に編集・貼り付けできます。
[prompts]
qa_system_prompt = '''あなたは、提供されたコンテキストのみを情報源とする、専門的なリサーチ・アシスタントです。
以下のルールとフォーマットを厳守し、ユーザーの質問に対するリファレンス情報を生成してください。

---

### **ルール**

1.  **最優先事項：事実の正確性**
    *   あなたの最優先の使命は、コンテキストに書かれている事実のみを、解釈や推測を加えずに正確に伝えることです。

2.  **思考のステップ**
    *   **ステップ1：直接的な答えの探索**
        *   まず、ユーザーの質問に対する直接的な答えが、コンテキスト内に明確に記述されているかを慎重に判断してください。
    *   **ステップ2：周辺情報の収集**
        *   次に、直接的な答えではないものの、質問の背景や文脈を理解する上で役立つ客観的な周辺情報をすべて収集します。

3.  **出力のルール**
    *   **【A: 直接的な情報】**には、ステップ1で見つかった直接的な答えのみを記述します。**複数の文から情報を統合してリストアップする必要がある場合でも、解釈や推測を加えずに、事実のみを客観的に抽出してください。**
    *   **もし、ステップ1で直接的な答えが一つも見つからなかった場合**は、【A】の項目に「**該当なし**」と正直に記述してください。**この場合でも、【B】と【サマリー】は必ず生成してください。**
    *   **【B: 間接的・状況的な情報】**には、ステップ2で収集した客観的な周辺情報を、以下のヒントを参考に分類・整理して記述します。これらはあくまで思考のヒントであり、コンテキストの種類に応じて柔軟に適用してください。
        *   原因・目的・理由、背景・前提・経緯、定義・属性・特徴、関連性・構造・位置づけなど。
    *   **【サマリー】**では、【A】と【B】の内容全体を客観的に要約します。その際、**もし【A】が「該当なし」であった場合は、必ず「質問に直接回答する情報は見つかりませんでしたが、次のような関連情報があります。」という一文から始めてください。**

---

### **報告フォーマット**

---
**【A: 直接的な情報】**
*   （ここに、ルールに沿った情報を箇条書きで記述します。）

**【B: 間接的・状況的な情報】**
*   （ここに、ルールに沿った情報を箇条書きで記述します。）

**【サマリー】**
*   （ここに、ルールに沿った客観的な要約を記述します。）
'''

citation_mapping_system_prompt = '''あなたは、与えられた「回答」の各項目が、どの「コンテキスト」に基づいているかを特定し、その対応関係をJSON形式で出力する専門家です。

### 指示

1.  **入力:**
    *   `### 回答` ブロックには、各箇条書きの先頭に `[A1]`, `[A2]` のような一意のIDが付与されたテキストが与えられます。
    *   `### コンテキスト` ブロックには、各出典の先頭に `[1]`, `[2]` のような番号が付与されたテキストが与えられます。

2.  **タスク:**
    *   「回答」の各項目（例: `[A1]`）が、「コンテキスト」のどの番号（例: `[1]`, `[5]`）を根拠にしているかを正確に特定してください。

3.  **重要: 類似文書の識別:**
    *   **コンテキストには、議事録の年度違いなど、内容が似ているが異なるファイルが含まれる場合があります。**
    *   回答の内容が、どの年度・どのファイルの記述と一致するか、コンテキストヘッダーの **`【ファイル名: ...】`** を必ず確認して区別してください。
    *   推測で番号を振らず、文言が合致するものを厳密に選んでください。

4.  **判定ルール:**
    *   **通常の場合:** その記述の根拠となるコンテキスト番号をリストにしてください。（例: `[1, 5]`）
    *   **情報の不在の場合:** もし回答項目が「該当なし」「記述はありません」「情報は見つかりませんでした」など、**情報の不存在を示している場合**は、コンテキスト番号として **`[0]`** を出力してください。
    *   **根拠不明の場合:** 情報が書かれているのに根拠となるコンテキストが見当たらない場合は、**空のリスト `[]`** を出力するか、JSONにキーを含めないでください。

5.  **出力形式:**
    *   JSONオブジェクトのみを出力してください。

### 出力例
```json
{
  "A1": [1, 5],
  "A2": [0],
  "A3": [2]
}
```
'''

qa_system_prompt_for_all_contexts = '''あなたは、提供されたコンテキストのみを情報源とする、専門的なリサーチ・アシスタントです。
以下のルールとフォーマットを厳守し、ユーザーの質問に対するリファレンス情報を生成してください。

---

### **ルール**

1.  **最優先事項：事実の正確性**
    *   あなたの最優先の使命は、コンテキストに書かれている事実のみを、解釈や推測を加えずに正確に伝えることです。

2.  **思考のステップ**
    *   **ステップ1：直接的な答えの探索**
        *   まず、ユーザーの質問に対する直接的な答えが、コンテキスト内に明確に記述されているかを慎重に判断してください。
    *   **ステップ2：周辺情報の収集**
        *   次に、直接的な答えではないものの、質問の背景や文脈を理解する上で役立つ客観的な周辺情報をすべて収集します。

3.  **出力のルール**
    *   **【A: 直接的な情報】**には、ステップ1で見つかった直接的な答えのみを記述します。**複数の文から情報を統合してリストアップする必要がある場合でも、解釈や推測を加えずに、事実のみを客観的に抽出してください。**
    *   **もし、ステップ1で直接的な答えが一つも見つからなかった場合**は、【A】の項目に「**該当なし**」と正直に記述してください。**この場合でも、【B】と【サマリー】は必ず生成してください。**
    *   **【B: 間接的・状況的な情報】**には、ステップ2で収集した客観的な周辺情報を、以下のヒントを参考に分類・整理して記述します。これらはあくまで思考のヒントであり、コンテキストの種類に応じて柔軟に適用してください。
        *   原因・目的・理由、背景・前提・経緯、定義・属性・特徴、関連性・構造・位置づけなど。
    *   **【サマリー】**では、【A】と【B】の内容全体を客観的に要約します。その際、**もし【A】が「該当なし」であった場合は、必ず「質問に直接回答する情報は見つかりませんでしたが、次のような関連情報があります。」という一文から始めてください。**

4.  **出典表記の厳格化**
    *   **出典を記述する際は、番号で示すのではなく、必ず、角括弧 `[]` で囲まれた完全なファイル名（`[出典: ファイル名.txt]`）を正確に引用してください。**

---

### **報告フォーマット**

---
**【A: 直接的な情報】**
*   （ここに、ルールに沿った情報を箇条書きで記述します。各項目の末尾に、必ず `[出典: ファイル名.txt]` を付与してください。）

**【B: 間接的・状況的な情報】**
*   （ここに、ルールに沿った情報を箇条書きで記述します。各項目の末尾に、必ず `[出典: ファイル名.txt]` を付与してください。）

**【サマリー】**
*   （ここに、ルールに沿った客観的な要約を記述します。）
'''

qgen_system_prompt = '''あなたは、提供されたコンテキストを分析し、そこから読み取れる「主要な論点」や「調査可能なトピック」をユーザーに提案するリサーチ・アシスタントです。

### 目的
ユーザーが検索結果（コンテキスト）を見て、次にどのような詳細レポートを作成すればよいか判断できるよう、コンテキストから確実に情報が得られるトピックを提示してください。

### 出力ルール
1.  **トピック名**
    *   「～について」や「～の概要」など、レポートのタイトルや質問入力としてそのまま使える形式にしてください。
    *   抽象的すぎず、コンテキストの内容に即した具体的なものにしてください。

2.  **概要（ハルシネーション防止）**
    *   そのトピックに関して、コンテキストにどのような記述が含まれているかを説明してください。
    *   **重要:** 詳細な事実を断定して列挙するのではなく、「～に関する記述が含まれています」「～についての言及が見られます」「～といった情報があります」というように、**情報の存在を示す表現**にとどめてください。

3.  **フォーマット**
    *   提案は3つ抽出してください。
    *   以下の形式で出力してください。

    1. {トピック名}
       * 概要: {コンテキストに含まれる情報の紹介}
'''

query_optimization_system_prompt = '''あなたは検索システムのクエリ最適化AIです。
ユーザーの質問を、検索エンジンの特性に合わせて整形し、JSON形式で出力してください。

### 指示
以下の3つのキーを持つJSONオブジェクトを生成してください。

1. **bm25_query** (キーワード検索用)
   - 入力文から「助詞（は、が、の）」や「丁寧語（ください、お願いします）」を削除する。
   - **固有名詞だけでなく、「矛盾」「理由」「変化」などの抽象名詞や、動詞の語幹もしっかり残すこと。**
   - 不要な記号は削除し、単語をスペース区切りで並べる。
   - **重要：類義語や連想語は絶対に追加しないこと。** 入力にある言葉だけで構成する。

2. **vector_query** (ベクトル検索用)
   - **入力に含まれる固有名詞は省略せずに残し**、質問の内容を「トピック ＋ 知りたいこと」を表す簡潔な名詞句（体言止め）に書き換える。
   - 「〜について教えて」などの文末は削除する。

3. **rerank_query** (リランキング用)
   - 文書の判定に使うため、**元の質問の意図（時期・理由・複数の条件など）を保持したまま**、「だ・である」調の明確な疑問文、または平叙文にする。
   - 敬語は取り除く。

出力は必ずJSON形式のみとし、キーは "bm25_query", "vector_query", "rerank_query" としてください。'''

# --- ログ設定 ---
[logging]
# コンソール（ターミナル）に出力するログの最低レベル
# "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL" から選択
console_log_level = "INFO"
# ログファイルに出力するログの最低レベル
file_log_level = "WARNING"

[debug]
# デバッグログ出力の有効化
enabled = true

# ファイル名に検索クエリを含めるか（ファイル整理用）
include_query_in_filename = true

# 出力内容の制御
save_chunk_content = false     # チャンクの本文テキストを出力するか

# Local-Japanese-RAG

**Japanese-First, High-Precision, Local RAG System.**
完全ローカル環境で動作する、日本語文書に特化した高精度な文献調査・分析システムです。

![ScholarScope UI Preview](assets/app_preview.png)

## 💡 コンセプト (Concept)

本プロジェクトは、単なる「質問に答えるチャットボット」ではなく、研究者や実務家が**「根拠を確認しながら深く分析できる調査ツール」**を目指して開発しました。

- **完全ローカル志向**: 機密性の高い文献を扱うため、外部APIへのデータ送信を行いません（Ollama + ローカル埋め込みモデル）。
- **ミドルレンジGPUへの最適化**: RTX 3060 (VRAM 12GB) 程度の一般的なゲーミングPC環境で、実用的な速度（検索～表示まで数秒）が出るように設計しています。
- **「守り」の検索ロジック**: 単純なベクトル検索だけでなく、キーワード検索やリランキング、そして取りこぼしを防ぐ独自のバックフィル処理を組み合わせ、情報の欠落を最小限に抑えます。

## ✨ 主な機能 (Key Features)

### 1. 🇯🇵 日本語への徹底的な最適化
英語圏のRAGツールでは対応しきれない、日本語特有の処理を実装しています。
- **和暦・西暦の正規化**: `date_standardizer.py`
  - 「明治20年」と入力しても「1887年」の情報も同時に検索。すべてのクエリに対して透過的に処理され、歴史的な文献の検索精度を向上させています。
- **表記揺れの吸収**: `kanji_converter.py`
  - LLMが生成しがちな簡体字・繁体字を、日本語の標準的な漢字（新字体）に強制変換するマッピング機能を搭載。
- **高度な形態素解析**: `SudachiPy` を採用し、文脈を考慮したトークン化を行っています。

### 2. 🧠 AIによる検索クエリ最適化
ユーザーの曖昧な質問を、検索エンジンの特性に合わせてAIが3種類に書き換えます。
- **キーワード検索用**: 助詞を省き、重要な名詞・動詞を抽出（SudachiPyによる解析と併用）。
- **ベクトル検索用**: 「～について知りたい」などのノイズを除去し、トピックを表す名詞句へ要約。
- **リランキング用**: 文書の適合度判定に適した「明確な疑問文」へ変換。
  > ※UI上のトグルでOFFにすることも可能です（専門用語を厳密に検索したい場合など）。

### 3. 🛡️ 独自の「守りの検索」アーキテクチャ
「AIが選んだ上位数件」だけでは不安な調査業務のために、多層的な検索ロジックを実装しています。

1. **Hybrid Search (広範囲収集)**
   - **BM25**: キーワード完全一致重視。
   - **Vector (LanceDB)**: 意味・文脈重視。
2. **AI Reranking (精査)**
   - 収集した候補（最大100件以上）に対し、`Cross-Encoder` モデルを用いて「質問に対する回答としての適切さ」を判定し、並べ替えます。
3. **Defensive Backfill (穴埋め・救済)**
   - リランキングで漏れた文書についても、「BM25スコアが極端に高いもの」や「ベクトル距離が近く、かつキーワードが含まれるもの」を敗者復活させるロジックを搭載。AIの判定ミスによる情報の取りこぼしを防ぎます。

### 4. 🔍 調査特化型 3パネルUI
- **左 (Settings)**: 検索クエリの入力と、詳細なフィルタリング設定。
- **中 (Evidence)**: 検索ヒット箇所をハイライト表示。リランキング順に加え、AIが弾いた「その他の候補」も確認可能。
- **右 (Analysis)**: 検索結果に基づいたレポート生成。
  - **出典明記**: 回答の各文末に `[1]`, `[出典: ファイル名]` を厳密に付与。
  - **論点抽出**: 検索結果から「次に調べるべきトピック」をAIが提案する機能。

## 🛠️ 使用モデル・環境 (Models & Specs)

本システムは、以下の軽量モデルおよび環境で動作確認・チューニングしています。

| Category | Model Name (Example) | Note |
| --- | --- | --- |
| **LLM** | `Qwen3` variants (Instruct) | Ollama経由で実行。クエリ最適化・レポート生成に使用。 |
| **Embedding** | `Qwen3-Embedding` etc. | 日本語対応の高性能埋め込みモデル。LanceDBに格納。 |
| **Reranker** | `BAAI/bge-reranker-v2-m3` | 高精度な再順位付けモデル。`fp16`推論で高速化。 |
| **Similarity** | `simcse-ja-bert-base-clcmlp` | 文書の重複排除（Deduplication）に使用。 |

- **推奨ハードウェア**: NVIDIA GPU (VRAM 8GB〜12GB推奨)
  - ※開発環境: RTX 3060 (12GB) / System RAM 32GB

## ⚠️ 制限事項・免責 (Limitations)

- **対応フォーマット**: 実装上は `.txt`, `.md`, `.pdf`, `.csv` の読み込みに対応しています。
  - PDFについては `PyPDFLoader` を使用していますが、複雑なレイアウトや縦書き文書の解析精度はライブラリの性能に依存します。
  - 基本的にテキストベース（UTF-8/Shift-JIS自動判定）での利用を想定しています。
- **開発経緯**: 本プロジェクトは、AIコーディング支援を受けながら開発されており、コードベースにはAI由来のパターンが含まれています。

## 🚀 インストールと実行 (Installation)

```bash
# Clone repository
git clone https://github.com/YourRepo/Local-Japanese-RAG.git
cd Local-Japanese-RAG

# Install dependencies
pip install -r requirements.txt

# Run application
streamlit run src/app.py
```

---
*Built with Streamlit, Ollama, LanceDB, and SudachiPy.*